---
# time docker-compose -f docker-compose-gpu.yaml build

services:
  private-gpt-gpu:
    container_name: privategpt-gpu
    hostname: privategpt-gpu
    image: neofob/privategpt:gpu
    build:
      context: .
      dockerfile: ./Dockerfile.local.gpu
      args:
        - LLAMA_CPP_PYTHON_VERSION=0.2.74
        - CMAKE_ARGS='-DLLAMA_CUDA=on -DLLAMA_CUDA_FORCE_MMQ=on'
    volumes:
      - ./local_data/:/home/worker/app/local_data
      - ./models/:/home/worker/app/models
      - ./settings.yaml:/home/worker/app/settings.yaml:ro
    entrypoint: ["/home/worker/app/.venv/bin/python", "-m" , "private_gpt"]
    #entrypoint: ["tail", "-f", "/dev/null"]
    ports:
      - 8001:8080
    environment:
      PORT: 8080
      PGPT_PROFILES: default
      PGPT_MODE: local
      GPU_LAYERS: 16
      CUDA_VISIBLE_DEVICES: "0"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
